# Multi-LLM Integration System

> **üìç Navigation**: [Root](../../README.md) ‚Üí [agentic-development](../README.md) ‚Üí [llm-integration](./README.md)

## Overview

This system enables intelligent routing of LLM requests across multiple providers to optimize for cost, performance, and specialization. The architecture reduces token costs by 60-85% while maintaining quality through strategic model selection.

## Core Components

### 1. LiteLLM Router
Central proxy server managing all LLM communications with:
- Unified OpenAI-compatible API interface
- Automatic fallback chains for reliability
- Semantic caching for 68% token reduction
- Real-time usage tracking and optimization

### 2. Model Tiers

#### Premium Tier (Complex Tasks)
- **Claude 4 Opus**: System architecture, cross-repository integration
- **Claude 4 Sonnet**: Multi-agent orchestration, high-context work
- **OpenAI GPT-4**: Specialized tools, backup for critical tasks

#### Standard Tier (General Development)
- **DeepSeek V3/R1**: 85% HumanEval, free via OpenRouter
- **Gemini 2.0 Flash**: 1M context, 200 requests/day free
- **Qwen 2.5 Coder**: 78% HumanEval, specialized coding

#### Specialist Tier (Domain-Specific)
- **Llama Vision**: Free image analysis via Together AI
- **Qwen QwQ**: Mathematical reasoning
- **Mistral Models**: Fast inference, Apache 2.0 licensed

## Quick Start

### 1. Install LiteLLM
```bash
pip install 'litellm[proxy,redis]'
```

### 2. Set Environment Variables
```bash
# Copy example environment file
cp .env.example .env

# Add your API keys
code .env
```

### 3. Start Router
```bash
# Development mode
litellm --config config/litellm.yaml --port 4000

# Production with Docker
docker-compose up -d
```

### 4. Test Integration
```bash
# Run health check
curl http://localhost:4000/health

# Test routing
python scripts/test_routing.py
```

## Configuration Files

- `config/litellm.yaml` - Main router configuration
- `config/models.yaml` - Model definitions and capabilities
- `config/routing_rules.yaml` - Task complexity routing logic
- `.env.example` - Required environment variables

## Integration with Agents

Each agent can leverage the multi-LLM system by:
1. Pointing to the LiteLLM proxy endpoint
2. Specifying task complexity in requests
3. Using semantic caching for similar queries
4. Implementing fallback handling

## Monitoring & Optimization

### Metrics Tracked
- Token usage per model/provider
- Cost per task type
- Response latency by model
- Cache hit rates
- Fallback frequency

### Optimization Strategies
1. **Semantic Caching**: Reuse responses for similar queries
2. **Batch Processing**: Group similar requests for efficiency
3. **Dynamic Routing**: Adjust model selection based on real-time performance
4. **Progressive Complexity**: Start with cheaper models, escalate as needed

## Cost Analysis

| Task Type | Traditional (Claude Only) | Multi-LLM System | Savings |
|-----------|---------------------------|------------------|----------|
| Documentation | $0.075/1K tokens | $0.0001/1K tokens | 99.8% |
| Simple Coding | $0.075/1K tokens | $0.00/1K tokens | 100% |
| Complex Architecture | $0.075/1K tokens | $0.075/1K tokens | 0% |
| Image Analysis | $0.075/1K tokens | $0.00/1K tokens | 100% |
| **Average Weighted** | **$0.075/1K** | **$0.015/1K** | **80%** |

## Troubleshooting

### Common Issues
1. **Rate Limits**: Check provider quotas in `config/rate_limits.yaml`
2. **API Keys**: Verify all keys in `.env` are valid
3. **Fallback Loops**: Review `config/routing_rules.yaml` for circular dependencies
4. **Cache Misses**: Adjust similarity threshold in Redis configuration

## Next Steps

1. [ ] Deploy LiteLLM proxy to production environment
2. [ ] Integrate semantic caching with Redis
3. [ ] Set up monitoring dashboards
4. [ ] Train custom router on Tuvens-specific workloads
5. [ ] Implement progressive task complexity analysis

## Resources

- [LiteLLM Documentation](https://docs.litellm.ai/)
- [Model Benchmarks](./benchmarks/README.md)
- [Cost Calculator](./scripts/cost_calculator.py)
- [Integration Examples](./examples/)
